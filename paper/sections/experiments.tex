\section{Experimental Evaluation}
\label{sec:experiments}

\subsection{Experimental Setup}

\textbf{Hardware.} All experiments were conducted on a system with an Intel 11th-Gen CPU (8 cores), 16 GB RAM, and an NVIDIA GeForce RTX 3050 Ti Laptop GPU (4 GB GDDR6, 2560 CUDA cores, 192 GB/s memory bandwidth). The GPU implementation uses CUDA~13.1 with CuPy~\cite{cupy2017} for NDS kernels and a dedicated \texttt{nvcc}-compiled CUDA kernel for crowding distance.

\textbf{Benchmark Problems.} We use the DTLZ~\cite{deb2005dtlz} benchmark suite with population sizes $N \in \{100, 500, 1000, 2000, 5000, 10000\}$ and objective counts $M \in \{2, 3, 5, 8, 10\}$.

\textbf{Baselines.} We compare against three \emph{optimized C++} baselines compiled with GCC 9.2.0 at \texttt{-O3}:
\begin{itemize}
    \item \textbf{C++ NSGA-II sort:} the classic $\mathcal{O}(MN^2)$ sort~\cite{deb2002fast};
    \item \textbf{C++ DCNS:} divide-and-conquer sort with sum-of-objectives pre-sort~\cite{mishra2016dcns};
    \item \textbf{C++ BOS:} Best Order Sort~\cite{kumar2016bos}.
\end{itemize}
Additionally, we include Python (NumPy) implementations of the same algorithms to demonstrate the importance of using compiled baselines for fair GPU comparisons.

\textbf{Metrics.} Runtime (wall-clock, ms, averaged over 10 trials), speedup ratio, and Pareto front correctness.

\subsection{NDS Speedup versus Population Size}

Fig.~\ref{fig:speedup_N} presents the core result: GPU-NDS speedup over optimized C++ baselines as a function of population size $N$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig1_nds_speedup.png}
    \caption{(a)~Runtime comparison of C++ baselines and GPU-NDS on DTLZ2 ($M\!=\!5$) in log scale. (b)~GPU speedup over C++ DCNS grows with both $N$ and $M$.}
    \label{fig:speedup_N}
\end{figure}

At $N\!=\!10{,}000$ and $M\!=\!10$, GPU-NDS achieves a $27.8\times$ speedup over C++ DCNS and $7.9\times$ over C++ BOS. The crossover point, where GPU-NDS becomes faster than all C++ baselines, is approximately $N\!=\!500$ for DCNS and $N\!=\!2{,}000$ for BOS. Unlike Aguilar-Rivera~\cite{aguilar2020gpu}, whose GPU NSGA-II reports higher speedups via stochastic non-dominated sorting (approximate front assignments), all speedups reported here are achieved with exact, provably correct front assignments (Proposition~2).

\subsection{Why C++ Baselines Matter}

Prior GPU sorting papers often compare against Python (NumPy) baselines, which inflates reported speedups by conflating interpreter overhead with algorithmic gains. Fig.~\ref{fig:py_vs_cpp} illustrates this effect: for $N\!=\!5{,}000$ and $M\!=\!5$, the speedup over Python DCNS is $8{,}173\times$, but the speedup over \emph{optimized C++} DCNS is only $12.5\times$ --- a difference of nearly three orders of magnitude.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/fig3_python_vs_cpp.png}
    \caption{GPU speedup versus Python baselines (inflated) vs. C++ baselines (honest). Using compiled baselines reduces reported speedup from $8{,}173\times$ to $12.5\times$ at $N\!=\!5{,}000$.}
    \label{fig:py_vs_cpp}
\end{figure}

\subsection{Scalability with Number of Objectives $M$}

Fig.~\ref{fig:scalability_M} shows runtime versus $M$ for fixed $N\!=\!2{,}000$. As $M$ increases, CPU algorithms slow down because each dominance check requires $M$ comparisons. GPU-NDS scales more gracefully due to memory coalescing in the tiled kernel: the per-comparison cost is amortised over the TILE$^2$ thread block.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig2_scalability_M.png}
    \caption{Runtime vs.\ $M$ ($N\!=\!2{,}000$) for DTLZ2 (left) and DTLZ6 (right). GPU-NDS maintains sub-10\,ms runtime even at $M\!=\!10$.}
    \label{fig:scalability_M}
\end{figure}

\subsection{End-to-End NSGA-II Comparison}

To measure the impact on a complete MOEA, we implement a GPU-resident NSGA-II where all operators (NDS, crowding distance, tournament selection, SBX crossover, polynomial mutation, objective evaluation) execute on the GPU as dedicated CUDA kernels. Data stays GPU-resident across generations to avoid PCIe transfer overhead.

The CPU baseline is a fully compiled C++ NSGA-II: all operators (NDS, crowding distance, tournament selection, SBX crossover, polynomial mutation) are implemented in C++ compiled with \texttt{-O3 -ffast-math}. No Python or NumPy is used in the CPU baseline.

Fig.~\ref{fig:end_to_end} compares total runtime of GPU-NSGA-II and C++-NSGA-II on DTLZ2 ($M\!=\!3$, 50--100 generations depending on problem size).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/fig4_end_to_end.png}
    \caption{End-to-end NSGA-II timing (DTLZ2, $M\!=\!3$) against a fully compiled C++ baseline. GPU-NSGA-II surpasses C++ at $N\!\geq\!2{,}000$ ($2.09\times$). At smaller populations, GPU kernel launch overhead dominates.}
    \label{fig:end_to_end}
\end{figure}

Against a fully compiled C++ NSGA-II baseline, the GPU advantage is $2.09\times$ at $N\!=\!2{,}000$ ($M\!=\!3$, 50 generations). For $M\!=\!5$, the crossover occurs at approximately $N\!=\!1{,}000$. At smaller populations ($N \leq 500$), GPU kernel launch overhead exceeds the computation time and the C++ baseline is faster. This crossover point is expected to shift to lower $N$ on more powerful GPUs with lower launch latency.

\textbf{Solution Quality.} To verify that GPU-NSGA-II converges correctly, we validate front assignments against the C++ baseline on the final generation. GPU-NSGA-II and C++-NSGA-II produce identical Pareto front assignments across all tested configurations ($N \in \{500, 1000, 2000\}$, $M = 3$, DTLZ2, 10 independent runs), as guaranteed by Proposition~\ref{prop:correctness} and validated by our 480-configuration correctness suite. The end-to-end speedup therefore does not come at the cost of solution quality.
\subsection{Ablation Study}

Fig.~\ref{fig:ablation} shows the effect of tile size and sum-of-objectives presort on GPU-NDS performance ($N\!=\!2{,}000$, $M\!=\!10$).

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig5_ablation.png}
    \caption{Ablation: (a) Tile size (16, 32, 64) shows similar performance. (b) Sum-of-objectives presort reduces runtime and variance.}
    \label{fig:ablation}
\end{figure}

Tile sizes 16 and 32 provide comparable performance; presort reduces mean runtime by $\sim$1.4\,ms (29\%) and substantially reduces variance. Both optimizations are enabled by default.

\subsection{Summary of Results}

Table~\ref{tab:summary} summarises peak GPU-NDS speedups over optimized C++ baselines.

\begin{table}[t]
\centering
\caption{Peak GPU-NDS Speedup over C++ Baselines}
\label{tab:summary}
\begin{tabular}{@{}lccccc@{}}
\toprule
C++ Baseline & $N$ & $M$ & C++ (ms) & Speedup & Crossover $N^*$ \\
\midrule
NSGA-II sort & 5,000 & 10 & 214.7 & $22.3\times$ & $\sim$500 \\
DCNS & 10,000 & 8 & 633.4 & $27.8\times$ & $\sim$500 \\
BOS & 10,000 & 10 & 182.8 & $7.9\times$ & $\sim$2{,}000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Reproducibility}

All source code, benchmark scripts, and raw experimental data are publicly available at: \texttt{[URL omitted for review]}. Experiments are fully reproducible using the provided \texttt{Makefile} and Python environment specification (\texttt{requirements.txt}). The C++ baselines are compiled with \texttt{g++ -O3 -ffast-math -std=c++17}; the GPU implementation requires CUDA~13.1, CuPy, and \texttt{nvcc} for the crowding distance kernel.
