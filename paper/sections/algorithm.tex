\section{GPU-NDS Algorithm}
\label{sec:algorithm}

\subsection{Overview}

GPU-NDS does not implement the recursive merge-tree structure of DCNS~\cite{mishra2016dcns,mishra2019dcns_journal} on GPU. Translating that recursion directly to GPU is non-trivial: the irregular subproblem sizes cause warp divergence and load imbalance, and recursive memory access undermines coalescing. Instead, GPU-NDS uses a dense tiled pairwise dominance matrix with two DCNS-inspired optimizations: (1) sum-of-objectives pre-sorting from Prakash et al.~\cite{prakash2024sumobj}, enabling dominance pruning via sum-bound skipping; and (2) TILE$\times$TILE shared-memory tiling that achieves the same memory-locality benefit as DCNS's divide-and-conquer partitioning, but in a GPU-friendly, branch-free manner. The result is worst-case $O(MN^2)$ arithmetic complexity---identical to the classical sort---but with (a) a factor-$T$ reduction in global memory traffic (Proposition~3) and (b) 30--60\% comparison reduction from sum-bound pruning (Section~\ref{sec:experiments}). We therefore describe GPU-NDS as \textit{DCNS-inspired} rather than a GPU implementation of DCNS.

GPU-NDS implements non-dominated sorting as a four-phase GPU pipeline (Fig.~\ref{fig:pipeline}), complemented by a dedicated crowding distance kernel for the complete GPU-resident NSGA-II:

\begin{enumerate}
    \item \textbf{Phase 1: Sum-of-Objectives Pre-Sort} --- Compute and sort solutions by the sum of their objective values, enabling dominance pruning.
    \item \textbf{Phase 2: Tiled Dominance Check} --- Perform pairwise dominance comparisons using tiled shared-memory kernels with sum-bound pruning.
    \item \textbf{Phase 3: Front Assignment} --- Iteratively assign solutions to fronts using GPU atomic operations.
    \item \textbf{Phase 4: Prefix Scan Cleanup} --- Use parallel scan operations for efficient index collection.
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fig6_algorithm_diagram.png}
    % TODO: regenerate figure to remove internal Figure 6 label
    \caption{GPU-NDS four-phase NDS pipeline with dedicated crowding distance kernel. Each phase is implemented as one or more CUDA kernels. All data remains GPU-resident between phases.}
    \label{fig:pipeline}
\end{figure}

\subsection{Phase 1: Sum-of-Objectives Pre-Sort}

We observe that if $\sum_{m=1}^{M} f_m(x_i) > \sum_{m=1}^{M} f_m(x_j)$, then $x_i$ \emph{cannot} dominate $x_j$ (since dominance requires $f_m(x_i) \leq f_m(x_j)$ for all $m$, which would imply $\sum f_m(x_i) \leq \sum f_m(x_j)$). By sorting solutions by their objective sum, we establish a monotonic ordering that enables early termination in the dominance check: for indices $i < j$ in the sorted order, only $x_i$ can potentially dominate $x_j$, not vice versa. The sum-of-objectives ordering for dominance pruning was formally analyzed by Prakash et al.~\cite{prakash2024sumobj}, whose result we adapt for GPU execution.

This pre-sort is computed on the GPU with $\mathcal{O}(N)$ work for the sum and $\mathcal{O}(N \log N)$ for the sort.

\subsection{Phase 2: Tiled Dominance Check}

The core kernel assigns a 2D grid of thread blocks, where each block handles a \texttt{TILE} $\times$ \texttt{TILE} tile of the $N \times N$ comparison matrix. Each block:

\begin{enumerate}
    \item Cooperatively loads \texttt{TILE} solutions from the ``row'' partition into shared memory (\texttt{tile\_A}).
    \item Cooperatively loads \texttt{TILE} solutions from the ``column'' partition into shared memory (\texttt{tile\_B}).
    \item Synchronises threads, then each thread $(tx, ty)$ checks whether \texttt{tile\_A[tx]} dominates \texttt{tile\_B[ty]}.
    \item \textbf{Sum-bound pruning:} If $\text{sum}[i] > \text{sum}[j]$, the thread returns immediately without performing the $M$-objective comparison.
    \item If dominance is confirmed, \texttt{atomicAdd(\&dom\_count[j], 1)} increments the domination counter.
\end{enumerate}

Shared memory tiling reduces global memory reads from $\mathcal{O}(N^2 M)$ to $\mathcal{O}((N/T)^2 \cdot T \cdot M)$ where $T$ is the tile size, because each solution's objectives are read from global memory once per tile row/column rather than once per comparison.

\subsection{Phase 3: Parallel Front Assignment}

Front assignment proceeds iteratively. In round $k$:
\begin{enumerate}
    \item A kernel scans all $N$ solutions; those with $\text{dom\_count}[i] = 0$ and not yet assigned are assigned $\text{rank}[i] = k$ and marked as newly assigned.
    \item A second kernel decrements $\text{dom\_count}[j]$ for all unassigned $j$ that were dominated by any newly assigned solution.
    \item Repeat for $k = 0, 1, 2, \ldots$ until all solutions are assigned.
\end{enumerate}

Both kernels use atomic operations for correctness: \texttt{atomicAdd} for domination counting and \texttt{atomicSub} for decrement.

\subsection{Phase 4: Prefix Scan}

We use CuPy~\cite{cupy2017} array operations (\texttt{cp.where}, boolean indexing) to efficiently collect indices of unassigned solutions between rounds, avoiding redundant work in subsequent front assignment rounds.

\subsection{Correctness}

Correctness of GPU-NDS (i.e., that it produces the same front assignment as sequential non-dominated sort) is established in Proposition~\ref{prop:correctness} (Section~\ref{sec:analysis}).

\begin{algorithm}
\caption{GPU-NDS: GPU Non-Dominated Sort}
\label{alg:gpu-nds}
\begin{algorithmic}[1]
\REQUIRE objectives$[N][M]$, TILE size $T$
\ENSURE rank$[N]$ (0-indexed front assignments)
\STATE \textbf{// Phase 1: Pre-sort}
\STATE sums $\leftarrow$ GPU\_sum(objectives, axis=1)
\STATE sort\_idx $\leftarrow$ GPU\_argsort(sums)
\STATE objectives $\leftarrow$ objectives[sort\_idx]
\STATE \textbf{// Phase 2: Tiled dominance}
\STATE dom\_count $\leftarrow$ zeros($N$)
\STATE Launch \textsc{TiledDominance} kernel: grid=$(N/T, N/T)$, block=$(T,T)$
\STATE \textbf{// Phase 3-4: Front assignment}
\STATE rank $\leftarrow$ $-1$ (unassigned)
\FOR{$k = 0, 1, 2, \ldots$}
    \STATE newly $\leftarrow$ \{$i$ : dom\_count[$i$]=0 $\wedge$ rank[$i$]=$-1$\}
    \IF{newly = $\emptyset$}
        \STATE \textbf{break}
    \ENDIF
    \STATE rank[newly] $\leftarrow k$
    \STATE \textsc{DecrementDom}(newly, dom\_count)
\ENDFOR
\STATE rank $\leftarrow$ rank[argsort(sort\_idx)] \COMMENT{restore original order}
\end{algorithmic}
\end{algorithm}

\subsection{GPU Crowding Distance Kernel}

Beyond the four-phase NDS pipeline, we implement
crowding distance assignment as a dedicated CUDA
kernel to complete the fully GPU-resident NSGA-II,
replacing the prior CuPy array-operation implementation
that incurred per-front Python loop overhead.

The kernel assigns one thread block per Pareto front.
Each block cooperatively collects front member indices
into shared memory, then for each objective~$m$:
(1) loads objective values into shared memory,
(2) performs an odd-even transposition sort within the
block, (3) assigns $\infty$ to boundary members, and
(4) accumulates normalised distance contributions for
interior members via \texttt{atomicAdd}.

For a front of size $f_s$ with $M$ objectives, the
kernel performs $O(f_s^2)$ work in the sort phase and
$O(f_s M)$ work in the accumulation phase, all in
shared memory with no global memory traffic except
the initial load and final write.
This reduces crowding distance time from
0.327\,ms to 0.109\,ms at $N=2{,}000$, $M=3$,
a $3\times$ improvement that ensures crowding distance
accounts for less than 2\% of total pipeline time.
